% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  man]{apa6}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}
\ifLuaTeX
\usepackage[bidi=basic]{babel}
\else
\usepackage[bidi=default]{babel}
\fi
\babelprovide[main,import]{english}
% get rid of language-specific shorthands (see #6817):
\let\LanguageShortHands\languageshorthands
\def\languageshorthands#1{}
% Manuscript styling
\usepackage{upgreek}
\captionsetup{font=singlespacing,justification=justified}

% Table formatting
\usepackage{longtable}
\usepackage{lscape}
% \usepackage[counterclockwise]{rotating}   % Landscape page setup for large tables
\usepackage{multirow}		% Table styling
\usepackage{tabularx}		% Control Column width
\usepackage[flushleft]{threeparttable}	% Allows for three part tables with a specified notes section
\usepackage{threeparttablex}            % Lets threeparttable work with longtable

% Create new environments so endfloat can handle them
% \newenvironment{ltable}
%   {\begin{landscape}\centering\begin{threeparttable}}
%   {\end{threeparttable}\end{landscape}}
\newenvironment{lltable}{\begin{landscape}\centering\begin{ThreePartTable}}{\end{ThreePartTable}\end{landscape}}

% Enables adjusting longtable caption width to table width
% Solution found at http://golatex.de/longtable-mit-caption-so-breit-wie-die-tabelle-t15767.html
\makeatletter
\newcommand\LastLTentrywidth{1em}
\newlength\longtablewidth
\setlength{\longtablewidth}{1in}
\newcommand{\getlongtablewidth}{\begingroup \ifcsname LT@\roman{LT@tables}\endcsname \global\longtablewidth=0pt \renewcommand{\LT@entry}[2]{\global\advance\longtablewidth by ##2\relax\gdef\LastLTentrywidth{##2}}\@nameuse{LT@\roman{LT@tables}} \fi \endgroup}

% \setlength{\parindent}{0.5in}
% \setlength{\parskip}{0pt plus 0pt minus 0pt}

% Overwrite redefinition of paragraph and subparagraph by the default LaTeX template
% See https://github.com/crsh/papaja/issues/292
\makeatletter
\renewcommand{\paragraph}{\@startsection{paragraph}{4}{\parindent}%
  {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
  {-1em}%
  {\normalfont\normalsize\bfseries\itshape\typesectitle}}

\renewcommand{\subparagraph}[1]{\@startsection{subparagraph}{5}{1em}%
  {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
  {-\z@\relax}%
  {\normalfont\normalsize\itshape\hspace{\parindent}{#1}\textit{\addperi}}{\relax}}
\makeatother

% \usepackage{etoolbox}
\makeatletter
\patchcmd{\HyOrg@maketitle}
  {\section{\normalfont\normalsize\abstractname}}
  {\section*{\normalfont\normalsize\abstractname}}
  {}{\typeout{Failed to patch abstract.}}
\patchcmd{\HyOrg@maketitle}
  {\section{\protect\normalfont{\@title}}}
  {\section*{\protect\normalfont{\@title}}}
  {}{\typeout{Failed to patch title.}}
\makeatother

\usepackage{xpatch}
\makeatletter
\xapptocmd\appendix
  {\xapptocmd\section
    {\addcontentsline{toc}{section}{\appendixname\ifoneappendix\else~\theappendix\fi\\: #1}}
    {}{\InnerPatchFailed}%
  }
{}{\PatchFailed}
\keywords{chaos, complexity, fractal, physiology, noise\newline\indent Word count: 3140}
\DeclareDelayedFloatFlavor{ThreePartTable}{table}
\DeclareDelayedFloatFlavor{lltable}{table}
\DeclareDelayedFloatFlavor*{longtable}{table}
\makeatletter
\renewcommand{\efloat@iwrite}[1]{\immediate\expandafter\protected@write\csname efloat@post#1\endcsname{}}
\makeatother
\usepackage{lineno}

\linenumbers
\usepackage{csquotes}
\usepackage[titles]{tocloft}
\cftpagenumbersoff{figure}
\renewcommand{\cftfigpresnum}{\itshape\figurename\enspace}
\renewcommand{\cftfigaftersnum}{.\space}
\setlength{\cftfigindent}{0pt}
\setlength{\cftafterloftitleskip}{0pt}
\settowidth{\cftfignumwidth}{Figure 10.\qquad}
\usepackage[labelfont=bf, font={scriptsize, color=gray}]{caption}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={The Structure of Chaos: An Empirical Comparison of Fractal Physiology Complexity Indices using NeuroKit2},
  pdfauthor={Dominique Makowski1, An Shu Te1, Tam Pham1, Zen J. Lau1, \& S.H. Annabel Chen1, 2, 3, 4},
  pdflang={en-EN},
  pdfkeywords={chaos, complexity, fractal, physiology, noise},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{\textbf{The Structure of Chaos: An Empirical Comparison of Fractal Physiology Complexity Indices using NeuroKit2}}
\author{Dominique Makowski\textsuperscript{1}, An Shu Te\textsuperscript{1}, Tam Pham\textsuperscript{1}, Zen J. Lau\textsuperscript{1}, \& S.H. Annabel Chen\textsuperscript{1, 2, 3, 4}}
\date{}


\shorttitle{Structure of Chaos}

\authornote{

Correspondence concerning this article should be addressed to Dominique Makowski, HSS 04-18, 48 Nanyang Avenue, Singapore (\href{mailto:dom.makowski@gmail.com}{\nolinkurl{dom.makowski@gmail.com}}) and Annabel Chen (\href{mailto:AnnabelChen@ntu.edu.sg}{\nolinkurl{AnnabelChen@ntu.edu.sg}}).

The authors made the following contributions. Dominique Makowski: Conceptualization, Data curation, Formal Analysis, Funding acquisition, Investigation, Methodology, Project administration, Resources, Software, Supervision, Validation, Visualization, Writing -- original draft; An Shu Te: Software, Project administration, Writing -- review \& editing; Tam Pham: Software, Writing -- review \& editing; Zen J. Lau: Software, Writing -- review \& editing; S.H. Annabel Chen: Supervision, Project administration, Writing -- review \& editing.

}

\affiliation{\vspace{0.5cm}\textsuperscript{1} School of Social Sciences, Nanyang Technological University, Singapore\\\textsuperscript{2} LKC Medicine, Nanyang Technological University, Singapore\\\textsuperscript{3} National Institute of Education, Singapore\\\textsuperscript{4} Centre for Research and Development in Learning, Nanyang Technological University, Singapore}

\abstract{%
Complexity quantification, through entropy, information theory and fractal dimension indices, is gaining a renewed traction in psychophsyiology, as new measures with promising qualities emerge from the computational and mathematical advances. Unfortunately, few studies compare the relationship and objective performance of the plethora of existing metrics, in turn hindering reproducibility, replicability, consistency, and clarity in the field. Using the NeuroKit2 Python software, we computed a list of 112 (predominantly used) complexity indices on signals varying in their characteristics (noise, length and frequency spectrum). We then systematically compared the indices by their computational weight, their representativeness of a multidimensional space of latent dimensions, and empirical proximity with other indices. Based on these considerations, we propose that a selection of 12 indices, together representing 85.97\% of the total variance of all indices, might offer a parsimonious and complimentary choice in regards to the quantification of the complexity of time series. Our selection includes \emph{CWPEn}, \emph{Line Length (LL)}, \emph{BubbEn}, \emph{MSWPEn}, \emph{MFDFA (Max)}, \emph{Hjorth Complexity}, \emph{SVDEn}, \emph{MFDFA (Width)}, \emph{MFDFA (Mean)}, \emph{MFDFA (Peak)}, \emph{MFDFA (Fluctuation)}, \emph{AttEn}. Elements of consideration for alternative subsets are discussed, and data, analysis scripts and code for the figures are open-source.
}



\begin{document}
\maketitle

\hypertarget{introduction}{%
\subsection{Introduction}\label{introduction}}

Complexity is an umbrella term for concepts derived from information theory, chaos theory, and fractal mathematics, used to quantify unpredictability, entropy, and/or randomness. Using these approaches to characterize physiological signals (a subfield commonly referred to as ``fractal physiology'' {[}1{]}) has shown promising results in the assessment and diagnostic of the state and health of living systems {[}2--4{]}.

Over the past few decades, there has been an important increase in the number of complexity indices {[}5{]}. Although these new procedures are usually mathematically well-defined and theoretically promising, limited empirical evidence is available to understand their similarities and differences {[}2,5{]}. Moreover, some of these methods are resource-intensive and require long computation times. This complicates their application with techniques that utilise high sampling-rates (e.g., M/EEG) and makes them impractical to implement in real-time settings - such as brain-computer interfaces {[}6,7{]}. As such, having empirical data about the computation time of various complexity indices would prove useful, for instance to guide their selection, especially in contexts where time or computational resources are constrained.

Additionally, the lack of a comprehensive open-source and user-friendly software for computing various complexity indices likely contributes to the scarcity of empirical comparisons {[}8{]}. Indeed, many complexity indices are only described mathematically in journal articles, with reusable code seldom made available, therefore limiting their further application and validation {[}5,8{]}. Even when available and open-source, the code implementations of complexity measures are typically found scattered across different packages or scripts, or embedded within a larger goal-directed framework (e.g., \emph{HCTSA}, a time-series comparison tool {[}9{]}). To address this lack of unified accessibility, we added a comprehensive set of complexity-related features to \emph{NeuroKit2}, a Python package for physiological signal processing {[}10{]}. In doing so, we hope to provide users with an easy-to-use software capable of computing a wide range of complexity measures. The code is designed to run quickly and efficiently, while still being written in pure Python (with the help of standard dependencies such as \emph{NumPy} or \emph{Pandas} {[}11,12{]}) to maximize its reusability, transparency, and correctness.

Leveraging this tool, the goal of this study is to empirically compare a large number of complexity indices, inspect how they relate to one another, and derive recommendations for indices selection. More specifically, we will quantify the complexity of various types of signals with varying degrees of noise using 112 of the predominantly used indices that are available for computation with \emph{NeuroKit2}. We will then project the results on a latent space through factor analysis, and review the indices that we find the most relevant and interesting in regards to their representation of the latent dimensions. This analysis will be complemented by hierarchical clustering. It should be noted that, even though this is one of the largest empirical comparison of complexity measures to date to our knowledge, the list of indices used is by no means exhaustive, with new indices constantly being developed, such as for instance \emph{symmetropy} {[}13{]}.

\hypertarget{methods}{%
\subsection{Methods}\label{methods}}

\begin{figure}
\centering
\includegraphics{./figures/signals-1.pdf}
\caption{\label{fig:signals}Different types of simulated signals, to which was added 5 types of noise (violet, blue, white, pink, and brown) with different intensities. For each signal type, the top row shows the signal with a minimal amount of noise, and the bottom row with a maximal amount of noise.}
\end{figure}

The Python script to generate the data can be found at \textbf{\url{https://github.com/DominiqueMakowski/ComplexityStructure}}.

We started by generating 6 types of signals, one random-walk, two oscillatory signals (with one made of harmonic frequencies that results in a self-repeating - fractal-like - signal), two complex signals derived from Lorenz systems (with parameters \(\sigma = 10, \beta = 2.5, \rho = 28\); and \(\sigma = 20, \beta = 2, \rho = 30\), respectively) and one EEG-like simulated signal. Each of these signals was iteratively generated at 6 different lengths (ranging from 500 to 3000 by 500 samples). The resulting vectors were standardized and each were added 5 types of \((1/f)^\beta\) noise (namely violet \(\beta=-2\), blue \(\beta=-1\), white \(\beta=0\), pink \(\beta=1\), and brown \(\beta=2\) noise). Each noise type was added at 128 different intensities (linearly ranging from 0.001 to 3). Examples of generated signals are presented in \textbf{Figure 1}.

The combination of these parameters resulted in a total of 23040 signal iterations. For each of them, we computed 112 complexity indices, as well as additional basic metrics such as the \emph{length} of the signal and its dominant \emph{frequency}. We also included a \emph{random} number at each iteration to ensure that our dimensionality analyses accurately discriminate this unrelated feature (as a manipulation check). The parameters used (such as the time-delay \(\tau\) or the embedding dimension) are documented in the data generation script. For a complete description of the various indices included, please refer to NeuroKit's documentation at \textbf{\url{https://neuropsychology.github.io/NeuroKit}}, in addition to the data generation script.

\hypertarget{results}{%
\subsection{Results}\label{results}}

The data analysis script and the data are fully available at \textbf{\url{https://github.com/DominiqueMakowski/ComplexityStructure}}. The analysis was performed in R using the \emph{easystats} collection of packages {[}14--17{]}. As the results are primarily presented graphically via the figures, the code to fully reproduce them is also included in the analysis script.

\hypertarget{computation-time}{%
\subsubsection{Computation Time}\label{computation-time}}

\begin{figure}
\centering
\includegraphics{./figures/time-1.pdf}
\caption{\label{fig:time}Median computation time difference between the different complexity indices algorithms, as well as variability as a function of signal lengths (represented by different line colors). The indices are grouped in sections (background color) according to their median computation time. Note that the time is expressed in arbitrary units as it is intended to convey differences, since the actual time would depend on the system specifications.}
\end{figure}

Firstly, one should note that the computation times presented in \textbf{Figure 2} are relative (in arbitrary units) and do not correspond to real times, as these would highly depend on the system specifications. Rather, the goal here was to convey some intuition on the differences between different classes of indices (using the same machine and the same language of implementation, i.e., Python). While it is possible that computational advances or improvements in the code efficiency might change some of these values, we believe that the ``big picture'' should remain fairly stable, as it is to a large extent driven by the inherent nature of the algorithms under consideration.

Despite the relative shortness of the signals considered (a few thousand points at most), the fully-parallelized data generation script took around 24 hours to run on a 48-cores machine. After summarizing and sorting the indices by computation time, the most striking result is the order of magnitude of difference between the fastest and slowest indices. Additionally, some indices are particularly sensitive to the signal length, a property which, in combination with their computational cost, led to indices being 100,000 times slower to compute than others.

In particular, multiscale indices were among the slowest to compute due to their iterative nature (a given index being computed multiple times on coarse-grained sub-series of the signal). Indices related to Recurrence Quantification Analysis (RQA) were also relatively slow and did not scale well with signal length.

\begin{figure}
\centering
\includegraphics{./figures/correlation-1.pdf}
\caption{\label{fig:correlation}Correlation matrix of complexity indices.}
\end{figure}

For the subsequent analyses, we removed statistically redundant indices (which absolute correlation was equal to 1.0), such as \emph{NLDFD} - identical to \emph{LL}, \emph{ShanEn (15)} - identical to \emph{ShanEn (9)}, and \emph{CREn (15)} - identical to \emph{CREn (9)}. This results in a pool of 112 indices.

\hypertarget{correlation}{%
\subsubsection{Correlation}\label{correlation}}

The Pearson correlation analysis revealed that complexity indices, despite their multitude and their conceptual specificities, do indeed share similarities. They form two major clusters that are easily observable (the blue and the red groups in \textbf{Figure 3}). That being said, these two anti-correlated groups are mostly indicative of the fact that some indices, by design, index the ``predictability'', whereas others, the ``randomness'', and thus are negatively related to one another. In order to extract finer groupings, further analysis procedures were applied below.

\hypertarget{factor-analysis}{%
\subsubsection{Factor Analysis}\label{factor-analysis}}

\begin{figure}
\centering
\includegraphics{./figures/loadings-1.pdf}
\caption{\label{fig:loadings}Factor loadings of the complexity indices, colored by the factor they represent the most (center). On the left, the median computation times and on the right, the archetypicity - the inverse of factor profile complexity (i.e., the extent to which each index is a pure representative of its dominant factor, which is low for indices that equally load on different factors).}
\end{figure}

The agreement procedure for the optimal number of factors suggested that the 112 indices can be mapped on a multidimensional space of 13 orthogonal latent factors, that we extracted using a \emph{varimax} rotation. We then took interest in the loading profile of each index, and in particular the latent dimension that it maximally relates to (see \textbf{Figure 4}). Below are a description of the factors that we found to be interpretable.

The first extracted factor is the closest to the largest amount of indices, and is positively loaded by indices that are sensitive to the deviation of consecutive differences (e.g., \emph{LL}, \emph{PFD (Mean)}) as well as indices that capture the amplitude of fluctuations (\emph{DispEn (fluctuation)}, \emph{MFDFA (Max)}). In line with this, this factor was negatively loaded by indices related to Detrended Fluctuation Analysis (\emph{DFA}), which tends to index the presence of long-term correlations and repetitions. As such, this latent factor might be associated with the predominance of short-term vs.~long-term unpredictability.

The second factor was strongly loaded by indices that measure the feature-richness of the signal's system (as most of them operate on a state-space decomposition). It was found to be positively related to \emph{SVDEn} and the Kozachenko-Leonenko differential entropy (\emph{KLEn}), and negatively to the RQA \emph{Recurrence Rate} and \emph{Hjorth} Complexity.

The third factor was loaded predominantly by permutation-based metrics (\emph{PEn}, \emph{WPEn}, \emph{BubblEn}, etc.). The fourth factor included multiscale indices, such as \emph{MSWPEn}. The fifth factor was strongly loaded by signal \emph{length}, and thus might not capture features of complexity \emph{per se}. Indices with the most relation to it were indices generally known to be sensitive to signal length, such as \emph{ApEn}. The sixth factor was loaded by indices in which the signal or the Poincaré plot was discretized via binning or gridding, respectively. The seventh factor was loaded by sign-based entropy increments, and the eighth by multiscale \emph{IncrEn} and multiscale \emph{PLZC}. The ninth factor was loaded by \emph{EnofEn} and Kolmogorov Entropy (\emph{K2En}). The tenth factor was loaded positively by the amount of noise, and negatively by multifractal indices such as \emph{MFDFA (Width)}, suggesting a sensitivity to regularity. Finally, as a manipulation check for our factorization method, the random vector did not load unto any factors.

\hypertarget{hierarchical-clustering-and-connectivity-network}{%
\subsubsection{Hierarchical Clustering and Connectivity Network}\label{hierarchical-clustering-and-connectivity-network}}

\begin{figure}
\centering
\includegraphics{./figures/ggm-1.pdf}
\caption{\label{fig:ggm}Correlation network of the complexity indices. Only the links where \textbar r\textbar{} \textgreater{} 0.6 are displayed.}
\end{figure}

\begin{figure}
\centering
\includegraphics{./figures/clustering-1.pdf}
\caption{\label{fig:clustering}Dendrogram representing the hierarchical clustering of the complexity indices.}
\end{figure}

For illustration purposes, we represented the correlation matrix as a connectivity graph (see \textbf{Figure 5}). We then ran a hierarchical clustering (with a Ward D2 distance) to provide additional information about the groups discussed above. Indeed, while the factor analysis will predominantly show indices that are the most representative of a given latent dimension, clustering will construct groups based on the multidimensional profile (what dimensions a given index loads positively on, and what other does it load negatively on). This allowed us to refine our recommendations for complimentary complexity indices (see \textbf{Figure 6}).

\hypertarget{indices-selection}{%
\subsubsection{Indices Selection}\label{indices-selection}}

\begin{figure}
\centering
\includegraphics{./figures/varexplained-1.pdf}
\caption{\label{fig:varexplained}Variance of the whole dataset of indices explained by the subselection. Each line represents a random number of selected variables. The green line represents the optimal order (i.e., the relative importance) that maximizes the variance explained. The dotted blue line represents the cumulative relative median computation time of the selected indices, and shows that MFDFA and multiscale indices are the most resource-costly algorithms.}
\end{figure}

The selection of a subset of indices was based on a set of considerations: 1) high loadings on one predominant latent dimension, with additional attention to the pattern of secondary loadings. For instance, an index with a positive factor 1 loading and a negative factor 2 loading could complement another index with a similar factor 1 loading, but a positive factor 2 loading. This was facilitated by 2) the hierarchical clustering dendrogram (see \textbf{Figure 6}), with which we attempted to extract indices from each (meaningful) higher order clusters. Items related to clusters that we determine as being largely explained by noise, length or other artifacts were omitted. 3) A preference for indices with relatively shorter computation times. This yielded a selection of 12 indices. Next, we computed the cumulative variance explained by this selection in respect to the entirety of indices examined, and derived the optimal order to maximize the variance explained (see \textbf{Figure 7}). The 12 included indices, representing 85.97\% of the variance of the whole dataset, were:

\begin{itemize}
\tightlist
\item
  \emph{CWPEn}: The Conditional Weighted Permutation Entropy is based on the difference of weighted entropy between that obtained at an embedding dimension \(m\) and that obtained at \(m+1\) {[}18{]}.
\item
  \emph{LL}: The Line Length index stems out of a simplification of Katz' fractal dimension (\emph{KFD}) algorithm {[}19{]} and corresponds to the average of consecutive absolute differences. It is equivalent to \emph{NDLFD}, the Fractal dimension via Normalized Length Density {[}20{]}. As it captures the amplitude 1-lag fluctuations, this index is likely sensitive to noise in the series.
\item
  \emph{BubbEn}: The Bubble Entropy is based on Permutation Entropy. It uses the \emph{Bubble sort} algorithm and counts the number of swaps each vector undergoes in the embedding space instead of ranking their order {[}21{]}.
\item
  \emph{MSWPEn}: The Multiscale Weighted Permutation Entropy is the entropy of weighted ordinal descriptors of the time-embedded signal computed at different scales obtained by a coarse-graining procedure {[}22{]}.
\item
  \emph{MFDFA (Max)} : The value of singularity spectrum \emph{D} corresponding to the maximum value of singularity exponent \emph{H}.
\item
  \emph{Hjorth}: Hjorth's Complexity is defined as the ratio of the mean frequency of the first derivative of the signal to the mean frequency of the signal {[}23{]}.
\item
  \emph{SVDEn}: The Singular Value Decomposition (SVD) Entropy quantifies the amount of eigenvectors needed for an adequate representation of the system {[}24{]}.
\item
  \emph{MFDFA (Width)}: The width of the multifractal singularity spectrum {[}25{]} obtained via Detrended Fluctuation Analysis (DFA).
\item
  \emph{MFDFA (Mean)} : The mean of the maximum and minimum values of singularity exponent \emph{H}, which quantifies the average fluctuations of the signal.
\item
  \emph{MFDFA (Peak)} : The value of the singularity exponent \emph{H} corresponding to peak of singularity dimension \emph{D}. It is a measure of the self-affinity of the signal, and a high value is an indicator of high degree of correlation between the data points.
\item
  \emph{MFDFA (Increment)}: The cumulative function of the squared increments of the generalized Hurst's exponents between consecutive moment orders {[}26{]}.
\item
  \emph{AttEn}: The Attention Entropy is based on the frequency distribution of the intervals between the local maxima and minima of the time series {[}27{]}.
\end{itemize}

Finally, we visualized the expected value of our selection of indices for different types of signals under different conditions of noise (see \textbf{Figure 8}). This confirmed that \emph{LL} was primarily driven by the noise intensity (which is expected, as they capture the variability of successive differences). The other indices appear capable of discriminating between the various types of signals (when the signal is not dominated by noise).

\begin{figure}
\centering
\includegraphics{./figures/models-1.pdf}
\caption{\label{fig:models}Visualization of the expected value of a selection of indices depending on the signal type and of the amount of noise.}
\end{figure}

\hypertarget{discussion}{%
\subsection{Discussion}\label{discussion}}

As the span and application of complexity science grows, a systematic approach to compare their ``performance'' becomes necessary to reinforce the clarity and structure of the field. The term \emph{performance} used here is to be understood in a relative sense, as any such endeavor faces the ``hard problem'' of complexity science: various objective properties of signals (e.g., short-term vs.~long-term variability, auto-correlation, information, randomness {[}28,29{]}) participate in forming overarching concepts such as ``complex'' and ``chaotic''. Indices that are sensitive to some of these objective properties are thus conceptually linked through such overarching frameworks. However, it remains unclear how these high-level concepts transfer back, in a top-down fashion, into a combination of lower-level features. As such, it is conceptually complicated to benchmark complexity measures against ``objectively'' complex vs.~non-complex signals. In other words, while we are aware that different objective signal characteristics can contribute to the ``complexity'' of a signal, there is not a one-to-one correspondence between the latter and the former.

To circumvent the aforementioned consideration, we adopted a paradigm where we generated different types of signals to which we systematically added distinct types - and amount - of perturbations. It should be noted that we did not seek to measure how complexity indices can discriminate between these signal types, nor did we attempt at mimicking real-life signals or scenarios. The goal was instead to generate enough variability to reliably map the relationships between the indices.

Our results empirically confirm the plurality of underlying components of complexity (although it is here defined somewhat circularly as what is measured by complexity indices), and more importantly show that complexity indices vary in their sensitivity to various orthogonal latent dimensions. However, the mostly descriptive interpretation of these dimensions is a limitation of the present investigation, and future studies are needed to investigate and discuss them in greater depth (for instance, by modulating specific properties of signals and measuring their impact on these latent dimensions).

Given the increasing role of complexity science as a field and the sheer number of complexity indices already published, our study aimed to empirically map the relationship between various indices and provide useful information to guide future researchers in their selection of complexity metrics. An example of indices subselection that encapsulates information about different underlying dimensions at a relatively low computational cost include \emph{CWPEn}, \emph{Line Length (LL)}, \emph{BubbEn}, \emph{MSWPEn}, \emph{MFDFA (Max)}, \emph{Hjorth Complexity}, \emph{SVDEn}, \emph{MFDFA (Width)}, \emph{MFDFA (Mean)}, \emph{MFDFA (Peak)}, \emph{MFDFA (Fluctuation)}, \emph{AttEn}. These indices might be complimentary in offering a parsimonious, yet comprehensive profile of the complexity of a time series. Moving forward, future studies are needed to validate, analyze and interpret the nature of the dominant sensitivities of various indices groups identified in the present work. In doing so, complexity findings in prospective studies can be more easily interpreted and integrated into new research and novel theories.

\hypertarget{acknowledgments}{%
\subsection{Acknowledgments}\label{acknowledgments}}

We would like to thank the contributors of NeuroKit2, as well as the people that developed and shared open-source code which helped implementing the complexity algorithms in NeuroKit2. In particular, the contributors and maintainers of packages such as \emph{nolds} {[}30{]}, \emph{AntroPy} {[}31{]}, \emph{pyEntropy}, and \emph{EntropyHub} {[}32{]}.

The study was funded partly by the Presidential Post-Doctoral Award to DM and Ministry of Education Academic Research Fund Tier 2 Grant (Project No.: MOE2019-T2-1-019) to AC. The authors declare no conflict of interest, and the funding sponsors had no role in the design, execution, interpretation or writing of the study.

\newpage

\hypertarget{references}{%
\subsection{References}\label{references}}

\hypertarget{refs}{}
\begin{CSLReferences}{0}{0}
\leavevmode\vadjust pre{\hypertarget{ref-bassingthwaighte2013fractal}{}}%
\CSLLeftMargin{1. }%
\CSLRightInline{Bassingthwaighte, J.B.; Liebovitch, L.S.; West, B.J. \emph{Fractal Physiology}; Springer, 2013;}

\leavevmode\vadjust pre{\hypertarget{ref-lau2021brain}{}}%
\CSLLeftMargin{2. }%
\CSLRightInline{Lau, Z.J.; Pham, T.; Annabel, S.; Makowski, D. Brain Entropy, Fractal Dimensions and Predictability: A Review of Complexity Measures for EEG in Healthy and Neuropsychiatric Populations. \textbf{2021}.}

\leavevmode\vadjust pre{\hypertarget{ref-ehlers1995chaos}{}}%
\CSLLeftMargin{3. }%
\CSLRightInline{Ehlers, C.L. Chaos and Complexity: Can It Help Us to Understand Mood and Behavior? \emph{Archives of General Psychiatry} \textbf{1995}, \emph{52}, 960--964.}

\leavevmode\vadjust pre{\hypertarget{ref-goetz2007}{}}%
\CSLLeftMargin{4. }%
\CSLRightInline{Goetz, S.J. Spatial Dynamics, Networks and Modelling ? Edited by Aura Reggiani and Peter Nijkamp. \emph{Papers in Regional Science} \textbf{2007}, \emph{86}, 523--524, doi:\href{https://doi.org/10.1111/j.1435-5957.2007.00128.x}{10.1111/j.1435-5957.2007.00128.x}.}

\leavevmode\vadjust pre{\hypertarget{ref-yang2013}{}}%
\CSLLeftMargin{5. }%
\CSLRightInline{Yang, A.C.; Tsai, S.-J. Is Mental Illness Complex? From Behavior to Brain. \emph{Progress in Neuro-Psychopharmacology and Biological Psychiatry} \textbf{2013}, \emph{45}, 253--257, doi:\href{https://doi.org/10.1016/j.pnpbp.2012.09.015}{10.1016/j.pnpbp.2012.09.015}.}

\leavevmode\vadjust pre{\hypertarget{ref-azami2017refined}{}}%
\CSLLeftMargin{6. }%
\CSLRightInline{Azami, H.; Rostaghi, M.; Abásolo, D.; Escudero, J. Refined Composite Multiscale Dispersion Entropy and Its Application to Biomedical Signals. \emph{IEEE Transactions on Biomedical Engineering} \textbf{2017}, \emph{64}, 2872--2879.}

\leavevmode\vadjust pre{\hypertarget{ref-manis2018}{}}%
\CSLLeftMargin{7. }%
\CSLRightInline{Manis, G.; Aktaruzzaman, M.; Sassi, R. Low Computational Cost for Sample Entropy. \emph{Entropy} \textbf{2018}, \emph{20}, 61, doi:\href{https://doi.org/10.3390/e20010061}{10.3390/e20010061}.}

\leavevmode\vadjust pre{\hypertarget{ref-flood2021}{}}%
\CSLLeftMargin{8. }%
\CSLRightInline{Flood, M.W.; Grimm, B. EntropyHub: An Open-Source Toolkit for Entropic Time Series Analysis. \emph{PLOS ONE} \textbf{2021}, \emph{16}, e0259448, doi:\href{https://doi.org/10.1371/journal.pone.0259448}{10.1371/journal.pone.0259448}.}

\leavevmode\vadjust pre{\hypertarget{ref-fulcher2017}{}}%
\CSLLeftMargin{9. }%
\CSLRightInline{Fulcher, B.D.; Jones, N.S. Hctsa: A Computational Framework for Automated Time-Series Phenotyping Using Massive Feature Extraction. \emph{Cell Systems} \textbf{2017}, \emph{5}, 527--531, doi:\href{https://doi.org/10.1016/j.cels.2017.10.001}{10.1016/j.cels.2017.10.001}.}

\leavevmode\vadjust pre{\hypertarget{ref-Makowski2021neurokit}{}}%
\CSLLeftMargin{10. }%
\CSLRightInline{Makowski, D.; Pham, T.; Lau, Z.J.; Brammer, J.C.; Lespinasse, F.; Pham, H.; Schölzel, C.; Chen, S.H.A. {NeuroKit}2: A Python Toolbox for Neurophysiological Signal Processing. \emph{Behavior Research Methods} \textbf{2021}, \emph{53}, 1689--1696, doi:\href{https://doi.org/10.3758/s13428-020-01516-y}{10.3758/s13428-020-01516-y}.}

\leavevmode\vadjust pre{\hypertarget{ref-harris2020array}{}}%
\CSLLeftMargin{11. }%
\CSLRightInline{Harris, C.R.; Millman, K.J.; Walt, S.J. van der; Gommers, R.; Virtanen, P.; Cournapeau, D.; Wieser, E.; Taylor, J.; Berg, S.; Smith, N.J.; et al. Array Programming with {NumPy}. \emph{Nature} \textbf{2020}, \emph{585}, 357--362, doi:\href{https://doi.org/10.1038/s41586-020-2649-2}{10.1038/s41586-020-2649-2}.}

\leavevmode\vadjust pre{\hypertarget{ref-mckinney2010data}{}}%
\CSLLeftMargin{12. }%
\CSLRightInline{McKinney, W. et al. Data Structures for Statistical Computing in Python. In Proceedings of the Proceedings of the 9th python in science conference; Austin, TX, 2010; Vol. 445, pp. 51--56.}

\leavevmode\vadjust pre{\hypertarget{ref-girault2022}{}}%
\CSLLeftMargin{13. }%
\CSLRightInline{Girault, J.-M.; Menigot, S. Palindromic Vectors, Symmetropy and Symmentropy as Symmetry Descriptors of Binary Data. \emph{Entrpy} \textbf{2022}, \emph{24}, doi:\href{https://doi.org/10.3390/e24010082}{10.3390/e24010082}.}

\leavevmode\vadjust pre{\hypertarget{ref-correlationArticle}{}}%
\CSLLeftMargin{14. }%
\CSLRightInline{Makowski, D.; Ben-Shachar, M.; Patil, I.; Lüdecke, D. Methods and Algorithms for Correlation Analysis in {R}. \emph{JOSS} \textbf{2020}, \emph{5}, 2306, doi:\href{https://doi.org/10.21105/joss.02306}{10.21105/joss.02306}.}

\leavevmode\vadjust pre{\hypertarget{ref-seeArticle}{}}%
\CSLLeftMargin{15. }%
\CSLRightInline{Lüdecke, D.; Patil, I.; Ben-Shachar, M.S.; Wiernik, B.M.; Waggoner, P.; Makowski, D. {see}: An {R} Package for Visualizing Statistical Models. \emph{Journal of Open Source Software} \textbf{2021}, \emph{6}, 3393, doi:\href{https://doi.org/10.21105/joss.03393}{10.21105/joss.03393}.}

\leavevmode\vadjust pre{\hypertarget{ref-parametersArticle}{}}%
\CSLLeftMargin{16. }%
\CSLRightInline{Lüdecke, D.; Ben-Shachar, M.; Patil, I.; Makowski, D. Extracting, Computing and Exploring the Parameters of Statistical Models Using {R}. \emph{JOSS} \textbf{2020}, \emph{5}, 2445, doi:\href{https://doi.org/10.21105/joss.02445}{10.21105/joss.02445}.}

\leavevmode\vadjust pre{\hypertarget{ref-modelbasedPackage}{}}%
\CSLLeftMargin{17. }%
\CSLRightInline{Makowski, D.; Lüdecke, D.; Ben-Shachar, M.S.; Patil, I. \emph{\href{https://CRAN.R-project.org/package=modelbased}{{modelbased}: Estimation of Model-Based Predictions, Contrasts and Means}}; 2022;}

\leavevmode\vadjust pre{\hypertarget{ref-unakafov2014conditional}{}}%
\CSLLeftMargin{18. }%
\CSLRightInline{Unakafov, A.M.; Keller, K. Conditional Entropy of Ordinal Patterns. \emph{Physica D: Nonlinear Phenomena} \textbf{2014}, \emph{269}, 94--102.}

\leavevmode\vadjust pre{\hypertarget{ref-esteller2001line}{}}%
\CSLLeftMargin{19. }%
\CSLRightInline{Esteller, R.; Echauz, J.; Tcheng, T.; Litt, B.; Pless, B. Line Length: An Efficient Feature for Seizure Onset Detection. In Proceedings of the 2001 conference proceedings of the 23rd annual international conference of the IEEE engineering in medicine and biology society; IEEE, 2001; Vol. 2, pp. 1707--1710.}

\leavevmode\vadjust pre{\hypertarget{ref-kalauzi2009extracting}{}}%
\CSLLeftMargin{20. }%
\CSLRightInline{Kalauzi, A.; Bojić, T.; Rakić, L. Extracting Complexity Waveforms from One-Dimensional Signals. \emph{Nonlinear biomedical physics} \textbf{2009}, \emph{3}, 1--11.}

\leavevmode\vadjust pre{\hypertarget{ref-bubblee2017}{}}%
\CSLLeftMargin{21. }%
\CSLRightInline{Manis, G.; Aktaruzzaman, M.; Sassi, R. Bubble Entropy: An Entropy Almost Free of Parameters. \emph{IEEE Transactions on Biomedical Engineering} \textbf{2017}, \emph{64}, 2711--2718, doi:\href{https://doi.org/10.1109/tbme.2017.2664105}{10.1109/tbme.2017.2664105}.}

\leavevmode\vadjust pre{\hypertarget{ref-fadlallah2013weighted}{}}%
\CSLLeftMargin{22. }%
\CSLRightInline{Fadlallah, B.; Chen, B.; Keil, A.; Príncipe, J. Weighted-Permutation Entropy: A Complexity Measure for Time Series Incorporating Amplitude Information. \emph{Physical Review E} \textbf{2013}, \emph{87}, 022911.}

\leavevmode\vadjust pre{\hypertarget{ref-hjorth1970eeg}{}}%
\CSLLeftMargin{23. }%
\CSLRightInline{Hjorth, B. EEG Analysis Based on Time Domain Properties. \emph{Electroencephalography and clinical neurophysiology} \textbf{1970}, \emph{29}, 306--310.}

\leavevmode\vadjust pre{\hypertarget{ref-roberts1999temporal}{}}%
\CSLLeftMargin{24. }%
\CSLRightInline{Roberts, S.J.; Penny, W.; Rezek, I. Temporal and Spatial Complexity Measures for Electroencephalogram Based Brain-Computer Interfacing. \emph{Medical \& biological engineering \& computing} \textbf{1999}, \emph{37}, 93--98.}

\leavevmode\vadjust pre{\hypertarget{ref-kantelhardt2002multifractal}{}}%
\CSLLeftMargin{25. }%
\CSLRightInline{Kantelhardt, J.W.; Zschiegner, S.A.; Koscielny-Bunde, E.; Havlin, S.; Bunde, A.; Stanley, H.E. Multifractal Detrended Fluctuation Analysis of Nonstationary Time Series. \emph{Physica A: Statistical Mechanics and its Applications} \textbf{2002}, \emph{316}, 87--114.}

\leavevmode\vadjust pre{\hypertarget{ref-faini2021multiscale}{}}%
\CSLLeftMargin{26. }%
\CSLRightInline{Faini, A.; Parati, G.; Castiglioni, P. Multiscale Assessment of the Degree of Multifractality for Physiological Time Series. \emph{Philosophical Transactions of the Royal Society A} \textbf{2021}, \emph{379}, 20200254.}

\leavevmode\vadjust pre{\hypertarget{ref-yang2020classification}{}}%
\CSLLeftMargin{27. }%
\CSLRightInline{Yang, J.; Choudhary, G.I.; Rahardja, S.; Franti, P. Classification of Interbeat Interval Time-Series Using Attention Entropy. \emph{IEEE Transactions on Affective Computing} \textbf{2020}.}

\leavevmode\vadjust pre{\hypertarget{ref-namdari2019}{}}%
\CSLLeftMargin{28. }%
\CSLRightInline{Namdari, A.; Li, Z.(Steven). A Review of Entropy Measures for Uncertainty Quantification of Stochastic Processes. \emph{Advances in Mechanical Engineering} \textbf{2019}, \emph{11}, 168781401985735, doi:\href{https://doi.org/10.1177/1687814019857350}{10.1177/1687814019857350}.}

\leavevmode\vadjust pre{\hypertarget{ref-xiong2017}{}}%
\CSLLeftMargin{29. }%
\CSLRightInline{Xiong, W.; Faes, L.; Ivanov, P.Ch. Entropy Measures, Entropy Estimators, and Their Performance in Quantifying Complex Dynamics: Effects of Artifacts, Nonstationarity, and Long-Range Correlations. \emph{Physical Review E} \textbf{2017}, \emph{95}, doi:\href{https://doi.org/10.1103/physreve.95.062114}{10.1103/physreve.95.062114}.}

\leavevmode\vadjust pre{\hypertarget{ref-scholzel2019nolds}{}}%
\CSLLeftMargin{30. }%
\CSLRightInline{Schölzel, C. \emph{\href{https://doi.org/10.5281/zenodo.3814723}{Nonlinear Measures for Dynamical Systems}}; Zenodo, 2019;}

\leavevmode\vadjust pre{\hypertarget{ref-vallat2022antropy}{}}%
\CSLLeftMargin{31. }%
\CSLRightInline{Vallat, R. \emph{\href{https://github.com/raphaelvallat/antropy}{AntroPy: Entropy and Complexity of (EEG) Time-Series in Python}}; GitHub, 2022;}

\leavevmode\vadjust pre{\hypertarget{ref-flood2021entropyhub}{}}%
\CSLLeftMargin{32. }%
\CSLRightInline{Flood, M.W.; Grimm, B. EntropyHub: An Open-Source Toolkit for Entropic Time Series Analysis. \emph{Plos one} \textbf{2021}, \emph{16}, e0259448.}

\end{CSLReferences}


\clearpage
\renewcommand{\listfigurename}{Figure captions}


\end{document}
